{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import transforms as T\nimport matplotlib.pyplot as plt","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"## Define path and load csv data"},{"metadata":{"trusted":true},"cell_type":"code","source":"root_to_train_img = \"../input/siim-isic-melanoma-classification/jpeg/\"\nroot_to_train_csv = \"../input/siim-isic-melanoma-classification/train.csv\"\ndf = pd.read_csv(root_to_train_csv, engine=\"python\")","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Have a look at the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Have a look at the data\n# plot 5 random images \nimport random\nimport glob\n# a = random.choices(glob.glob(root_to_train_img + \"train/*\"), k=5)\n# for i in a:\n#     print(i)\n#     img = cv2.imread(i)\n#     img_name = i.split(\"/\")[-1][:-4]\n#     print(img_name)\n#     plt.imshow(img)\n#     plt.show()\n#     print(f\"{img_name}\")\n#     print(\"-\"*100)","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess CSV Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(root_to_train_csv, engine=\"python\")\ndf.head()\n\ndf = df.drop(\"patient_id\", axis=1)\n\n# sex is binary so can be binarized\nprint(\"[INFO] Binarizing sex\")\ndef binarize_sex(row):\n    if row['sex'] == 'male':\n        return 0\n    else:\n        return 1\n\ndf['sex'] = df.apply(lambda x: binarize_sex(x), axis=1)\n\n# Take care of nan values in age\nprint(\"[INFO] Taking care of Age\")\ndf['age_approx'].value_counts().plot(kind='bar')\nx1 = df.loc[df[\"benign_malignant\"] == \"benign\", 'age_approx']\nx2 = df.loc[df[\"benign_malignant\"] == \"malignant\", 'age_approx']\n\nbenign_mean = np.nanmean(x1)\nbenign_std = np.nanstd(x1)\nmalignant_mean = np.nanmean(x2)\nmalignant_std = np.nanstd(x2)\n\ndef fillna(row):\n    if np.isnan(row['age_approx']):\n        if row['benign_malignant'] == 'benign':\n            return np.random.normal(loc=benign_mean, scale=benign_std)\n        else:\n            return np.random.normal(loc=malignant_mean, scale=malignant_std)\n    else:\n        return row['age_approx']\n    \n# encode age\ndef encodeage(row):\n    if row['age_approx'] >=40 and row['age_approx'] <=60:\n        return \"highly likely\"\n    elif row['age_approx'] <=25 or row['age_approx'] >=75:\n        return \"least likely\"\n    else:\n        return \"average\"\n\ndf['age_approx'] = df.apply(lambda x: fillna(x), axis=1)\n# From the image we can roughly make three broad classes\n#     1. >40 <60 -- highly likely\n#     2. <25 >75 -- least likely\n#     3. rest -- average likelihood\ndf['age'] = df.apply(lambda x: encodeage(x), axis=1)\ndf.drop(\"age_approx\", axis=1, inplace=True)\n\n# take care of anatom_site_general_challenge\n# nan values replaced by probabilistically choosing  \n# one of the distinct values.\nprint(\"[INFO] Taking care of anatom...\")\nx1 = df.loc[df[\"benign_malignant\"] == \"benign\", 'anatom_site_general_challenge']\nx2 = df.loc[df[\"benign_malignant\"] == \"malignant\", 'anatom_site_general_challenge']\ndistinct = list(set(df['anatom_site_general_challenge'].values))\ndistinct = distinct[1:]\ndi = dict()\nfor i in distinct:\n    di[i] = 0\n\nfor i in x1:\n    if i in di.keys():\n        di[i] += 1\n\ntot_sum = 0\nfor _, val in di.items():\n    tot_sum += val\nprob_benign = []\nfor i in distinct:\n    prob_benign.append(di[i]/tot_sum)\n\ndi2 = dict()\nfor i in distinct:\n    di2[i] = 0\n\nfor i in x2:\n    if i in di2.keys():\n        di2[i]+=1\n\ntot_sum = 0\nfor _, val in di2.items():\n    tot_sum += val\nprob_malignant = []\nfor i in distinct:\n    prob_malignant.append(di2[i]/tot_sum)\n\ndef fillna(row):\n    if isinstance(row['anatom_site_general_challenge'], float):\n        if row['benign_malignant'] == 'benign':\n            return np.random.choice(distinct, p=prob_benign)\n        else:\n            return np.random.choice(distinct, p=prob_malignant)\n        print(\"Not Both\")\n    else:\n        return row['anatom_site_general_challenge']\n\ndf['anatom_site_general_challenge'] = df.apply(lambda x: fillna(x), axis=1)\n\nsave_info = {\n    \"age\":{\n        \"benign_mean\" : benign_mean,\n        \"benign_std\" : benign_std,\n        \"malignant_mean\": malignant_mean,\n        \"malignant_std\" : malignant_std\n    },\n    \"anatom\":{\n        \"distinct\":distinct,\n        \"prob_benign\":prob_benign,\n        \"prob_malignant\":prob_malignant\n    },\n    \"desc\": \"\"\" There are two charachteristics stored here age and anatom in form of dictionaries.\\n \n    age has 4 keys benign_mean, benign_std, malignant_mean, malignant_std\\n\n    anatom has 3 keys distinct, prob_benign, prob_malignant.\"\"\"\n}\n\n# save the required files can later be used in converting test dataset\n# with open(\"../input/siim-isic-melanoma-classification/save_info\", \"wb\") as f:\n#     pkl.dump(save_info, f)\n\nprint(\"[INFO] One hot encoding\")\n# Get dummy variables\n# Preferrable use OneHotEncoding from sklearn, here i am using \n# get dummy variables by pandas\ndf = pd.concat([df, pd.get_dummies(df['anatom_site_general_challenge'])], axis=1).drop([\"anatom_site_general_challenge\"], axis=1)\ndf = pd.concat([df, pd.get_dummies(df['diagnosis'])], axis=1).drop([\"diagnosis\"], axis=1)\ndf = pd.concat([df, pd.get_dummies(df['age'])], axis=1).drop('age', axis=1)\ntarget = df[[\"target\"]]\ndf.drop([\"target\", \"benign_malignant\"], axis=1, inplace=True)\ndf = pd.concat([df, target], axis=1)","execution_count":15,"outputs":[{"output_type":"stream","text":"[INFO] Binarizing sex\n[INFO] Taking care of Age\n[INFO] Taking care of anatom...\n[INFO] One hot encoding\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAX0AAAEFCAYAAAAPCDf9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAUPklEQVR4nO3dcbCldX3f8feHFSmgGHAX3OwSl07WKNgJhi0ytW2sOGWtNthOTdc0smlxtmOxmjZTXdLO2Mx0M2vaJpFJcboV41KjzKpJ2Ggp0k0xkxaEjUFwWQiboLBlhVWJYsyQLn77x/OQnF7v7j3n8uy599zf+zVz5jznd87zvd/n3HM/57nPeZ7npKqQJLXhlKVuQJI0PYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDnrfUDSxk9erVtWHDhqVuQ5JmxurVq7n11ltvrarNc+9b9qG/YcMG9u/fv9RtSNJMSbJ6vnE370hSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IasuwPzpprw/bPjPW4L+9840nuRJJmj2v6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIbM3MFZQxrnQC8P8pK0krimL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWrI2KGfZFWS30/y6f72OUluS/JQf332yGOvTXIoyYNJrhgZvyTJff191yXJsIsjSTqRSdb03w0cHLm9HdhXVRuBff1tklwIbAEuAjYD1ydZ1c/zQWAbsLG/bH5O3UuSJjJW6CdZD7wR+NDI8JXA7n56N/DmkfGbqurpqnoYOARcmmQtcFZV3VFVBdw4Mo8kaQrGXdP/ZeA9wHdHxs6rqiMA/fW5/fg64NGRxx3ux9b103PHJUlTsmDoJ3kT8ERV/d6YNefbTl8nGJ/vZ25Lsj/J/qNHj475YyVJCxlnTf81wI8l+TJwE/C6JB8FHu832dBfP9E//jBw/sj864HH+vH184x/j6raVVWbqmrTmjVrJlgcSdKJLBj6VXVtVa2vqg10H9D+dlX9JLAX2No/bCtwcz+9F9iS5LQkF9B9YHtXvwnoqSSX9XvtXDUyjyRpCp7L+fR3AnuSXA08ArwFoKoOJNkD3A8cA66pqmf6ed4BfAQ4Hbilv0iSpmSi0K+q24Hb++mvA5cf53E7gB3zjO8HXjlpk5KkYXhEriQ1xNCXpIYY+pLUkKa/GH1Ifsm6pFngmr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQz72zzHgOH0knk2v6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhnlp5BfM0zZLmck1fkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGLBj6Sf5SkruSfDHJgSQ/14+fk+S2JA/112ePzHNtkkNJHkxyxcj4JUnu6++7LklOzmJJkuYzzpr+08DrquqHgYuBzUkuA7YD+6pqI7Cvv02SC4EtwEXAZuD6JKv6Wh8EtgEb+8vmAZdFkrSABUO/Ot/ub57aXwq4Etjdj+8G3txPXwncVFVPV9XDwCHg0iRrgbOq6o6qKuDGkXkkSVMw1jb9JKuS3AM8AdxWVZ8HzquqIwD99bn9w9cBj47MfrgfW9dPzx2f7+dtS7I/yf6jR49OsjySpBMYK/Sr6pmquhhYT7fW/soTPHy+7fR1gvH5ft6uqtpUVZvWrFkzTouSpDFMtPdOVf0xcDvdtvjH+0029NdP9A87DJw/Mtt64LF+fP0845KkKRln7501Sb6vnz4deD3wALAX2No/bCtwcz+9F9iS5LQkF9B9YHtXvwnoqSSX9XvtXDUyjyRpCsY5tfJaYHe/B84pwJ6q+nSSO4A9Sa4GHgHeAlBVB5LsAe4HjgHXVNUzfa13AB8BTgdu6S+SpClZMPSr6l7gVfOMfx24/Djz7AB2zDO+HzjR5wGSpJPII3IlqSF+c5YWNM43cIHfwiXNAtf0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDPLWypmqc0zR7imbp5HFNX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0JekhrifvmaW+/xLk3NNX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNWTD0k5yf5H8mOZjkQJJ39+PnJLktyUP99dkj81yb5FCSB5NcMTJ+SZL7+vuuS5KTs1iSpPmMs6Z/DPiZqnoFcBlwTZILge3AvqraCOzrb9PftwW4CNgMXJ9kVV/rg8A2YGN/2TzgskiSFrBg6FfVkar6Qj/9FHAQWAdcCezuH7YbeHM/fSVwU1U9XVUPA4eAS5OsBc6qqjuqqoAbR+aRJE3BRNv0k2wAXgV8Hjivqo5A98YAnNs/bB3w6Mhsh/uxdf303HFJ0pSMHfpJXgB8CvjpqvrWiR46z1idYHy+n7Utyf4k+48ePTpui5KkBYz1zVlJTqUL/F+rql/vhx9PsraqjvSbbp7oxw8D54/Mvh54rB9fP8/496iqXcAugE2bNs37xiANxW/gUkvG2XsnwA3Awar6xZG79gJb++mtwM0j41uSnJbkAroPbO/qNwE9leSyvuZVI/NIkqZgnDX91wBvA+5Lck8/9rPATmBPkquBR4C3AFTVgSR7gPvp9vy5pqqe6ed7B/AR4HTglv4iSZqSBUO/qn6X+bfHA1x+nHl2ADvmGd8PvHKSBiVJw/GIXElqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQ8b6jlxJ4/H7drXcuaYvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGLBj6ST6c5IkkXxoZOyfJbUke6q/PHrnv2iSHkjyY5IqR8UuS3Nffd12SDL84kqQTGWdN/yPA5jlj24F9VbUR2NffJsmFwBbgon6e65Os6uf5ILAN2Nhf5taUJJ1kC4Z+Vf0O8I05w1cCu/vp3cCbR8Zvqqqnq+ph4BBwaZK1wFlVdUdVFXDjyDySpClZ7Db986rqCEB/fW4/vg54dORxh/uxdf303HFJ0hQN/UHufNvp6wTj8xdJtiXZn2T/0aNHB2tOklr3vEXO93iStVV1pN9080Q/fhg4f+Rx64HH+vH184zPq6p2AbsANm3adNw3B2ml2rD9M2M97ss733iSO9FKs9g1/b3A1n56K3DzyPiWJKcluYDuA9u7+k1ATyW5rN9r56qReSRJU7Lgmn6SjwOvBVYnOQy8D9gJ7ElyNfAI8BaAqjqQZA9wP3AMuKaqnulLvYNuT6DTgVv6iyRpihYM/ap663Huuvw4j98B7JhnfD/wyom6kyQNyiNyJakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhiz21MqSZsQ4p2n2FM3tcE1fkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkM8DYOksXlKh9nnmr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGuIJ1yRN3ZAnbvMkcJNxTV+SGmLoS1JDpr55J8lm4APAKuBDVbVz2j1I0lzjbCaC2d9UNNU1/SSrgP8EvAG4EHhrkgun2YMktWzam3cuBQ5V1R9V1Z8BNwFXTrkHSWpWqmp6Pyz5B8Dmqnp7f/ttwKur6p1zHrcN2Nbf/CHgwQVKrwa+NlCbQ9Wyp+nXsqfp17Kn6dcap87XAKpq89w7pr1NP/OMfc+7TlXtAnaNXTTZX1WbnktjQ9eyp+nXsqfp17Kn6dd6rnWmvXnnMHD+yO31wGNT7kGSmjXt0L8b2JjkgiTPB7YAe6fcgyQ1a6qbd6rqWJJ3ArfS7bL54ao6MEDpsTcFTbGWPU2/lj1Nv5Y9Tb/Wc6oz1Q9yJUlLyyNyJakhhr4kNcTQl6SGGPqS1JCZO59+kpfTnbphHd2BXY8Be6vq4EqotRx7GrLWcuxpyFr2NP2ehrSM+zpvtKeqenyxtWZqTT/Je+nO1xPgLrr9/gN8PMn2Wa+1HHsastZy7GnIWvY0/Z5Gar48yXuTXJfkA/30KyasMWhfA/V0cZI7gduBXwD+PfC5JHcm+ZFJewKgqmbmAvwBcOo8488HHpr1WsuxJ5dvtpdvpffUz/de4B5gO/CT/WX7s2NL9FwN1dM9dOcnmzt+GfDFSZ+rqpq5zTvfBb4f+Mqc8bX9fbNeazn2NGSt5djTkLXsafo9AVwNXFRV/3d0MMkvAgeAcb+zY8i+hurpzKr6/NzBqrozyZkT9gTM3jb9nwb2JXkIeLQf+wHgB4F3Hneu2am1HHsastZy7GnIWvY0/Z5guLAesq+herolyWeAG0d6Oh+4CvjvE/YEzOARuUlOoTsv/zq67W2Hgbur6pmVUGs59jRkreXY05C17GlJetoM/Aowb1hX1djhOODyDdnTG/iLD5ef7WlvVf23SXr683qzFvqSNNeQbyIruSeYsb13TiTJp1dyreXY05C1lmNPQ9ayp5Nbp6q+W1V3VtWnquqT/fRg4bqYvqbQ07aFHzXPfCtlTT/J2qo6slJrLceehqy1HHsaspY9Tb+nvt6nq+pNA9QZ8rkaqqd/WlX/eeL5VkroS9JcQ7+JDGHSnoY+YGymNu/0H448O/2iJDckuTfJx9IdsTZJrRcl2ZnkgSRf7y8H+7HvW4q+Bu5pOS7fcv39DVJr4OVbsa+pIZ+nhUwYroM9VwP2NPiBbDMV+sDPj0z/R+AI8HfpnohJ/83ZAzwJvLaqXlxVLwb+Vj/2iSXqa8ieluPyLdff31C1hly+lfyaGvJ5GvJNZLDnasA3kKuBv1pVO6vqo/1lJ90HxFdP0tOfW8wRXUt1Ab4wMn3PnPvumbDWg4u572T2NXBPy3H5luvvb5BaAy/fin1NDfk8zVPvQ8C/A14K/AvgN5foubqV7qjcl4yMvaQfu22COg8AL51n/KWT9vTsZdYOzjo3yb+k+/fmrCSp/hlg8v9avpLkPcDu6k9e1K8V/BR/sV/ttPsasqfluHzL9fc3VK0hl28lv6aGfJ7m2lRVF/fTv5Rk6wTzDvlcbaiq948OVNVXgfcn+ScT1Bn6QLaZ27zzX4AXAi8AdgOrAZK8hO4cFZP4h8CL6U5e9GSSb9Cd1Ogc4MeXqK+5PT3Z9/TiRfS0HJdvuf7+hqo15PKdrNfUSnueoH8TSfIz9G8iI/dNknHPLt/tSb7xHJ+rryR5z+jmpSTn9dvox34Dqe4grpcBP0f338NngX8L/FBNcIDXqJnfeyfJjVV11QB1/gbddrL7quqzz7HWX+9rfWmAWv+1qt62iPleDTxQVd9McgbdyZ5+hO68Hz9fVd+csN6lQFXV3UkuBDb39cc+KvA4Pb0KuH+Ani7qezo4SU99nXcBv1FVk67NzVfr5XR7WXy+qr49Mr55kj/Sk9DTeuDO59jT84G3Av+nqv5Hkn8E/DW639+umnOemRPUOQ3YMlLnJ/o6ByepM1LvfXOGrq+qo/2byC9Mkg9JfhD4e3SnOjhGdxK2jy/itXk23ev7SuDcfvhxYC+ws6qenKTekGYq9JPsnWf4dcBvA1TVj01Q666qurSffjtwDfCbwN8Gfqu6D0sWW+udwG9MWmvg5TsA/HBVHUuyC/gT4FPA5f3435+g1vuAN9Cdq+k24NV0a0CvB26tqh2L7Ok7wCcH6ulS4HOT9tTX+ibd8/OHwMeAT1TV18adf6TOP6f73R8ELgbeXVU39/d9oarGPhXunJ4+3vd0dBE9vYvutT1ET79G93yfDnwTOJPudX45XZaMtSllpM4ZwB/TrfH/el+HqvqpcXsa42f946r61TEf+y7gTcDvAH+H7r+OJ+neBP5ZVd0+7Z5OisV8ELBUF+D3gY8CrwV+tL8+0k//6KS1RqbvBtb002fSre1PvRbwhQGX7+Bo3Tn3Tfqh4n3AKro/0m8BZ/XjpwP3znJPI6+rU+jepG8AjtKdzGor8MIJe3pBP70B2E8Xsv/fa2SGe7q3v34e3Vrrqv52JnwdDFJnzJ/1yKSvqX76DOD2fvoHJn2uhurpZFxm7YPcS4B3A/8a+FdVdU+SP62qzy2i1in9v2Cn0K2lHAWoqj9JcmyJam1iuOX70sgaxReTbKqq/UleBkz07zNwrLrDx7+T5A+r6lsAVfWnSSY5Y+By7Kmfrb5Lt730s0lOpfsv4q3AfwDWjFlnVfWbT6rqy0leC3wyyUvpAm3Wezql38RzJl0ovgj4BnAacOoS1AEgyb3HuwuYdL//5wHP9L28EKCqHumf/6XqaVAzFfr9H8EvJflEf/04i1+GFwG/R/dLqCQvqaqvJnkBk/8xDFJr4OV7O/CBJP8G+BpwR5JH6T5EevuEtf4syRlV9R26N16g2xeZyU4Tuxx7gjm/o+q2Ke8F9iY5fYI6X01ycVXd09f5dpI3AR8G/soK6OkGul0IV9GtmHwiyR/RfaHHTUtQ51nnAVfQbYoZFeB/T1DnQ8Dd6b6p6m8C7wdIsobuTWkpehrcTG3TnyvJG4HXVNXPDljzDOC8qnp4qWsNsXxJXgj8Zbo3j8O1iO/WTHJaVT09z/hqYG1V3TfLPSV5WVX9waQ9zFNnPd1/IF+d577XVNX/muWe+nm+H6CqHkt3kNHr6TZX3LUUdfpaNwC/WlW/O899H6uqn5ig1kXAK+h2wnhg0l5ORk9Dm+nQlyRNZtb205ckPQeGviQ1xNCXpIYY+pLUEENfkhry/wBqeV8wrlhqMwAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"# Custom Dataset Class for PyTorch Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\n\nclass SIIMTrainDataset(Dataset):\n    def __init__(self, root_img, df, transforms, shuffle):\n        self.img_files = glob.glob(root_img + \"train/*\")\n        self.df = df\n        self.idx = list(range(len(self.img_files)))\n        self.len = len(self.df)\n        \n        if shuffle:\n            self.shuffledidx = list(range(len(self.img_files)))\n            random.shuffle(self.shuffledidx)\n        \n        print(len(self.shuffledidx))\n        self.transforms = transforms\n            \n    def __len__(self):\n        return self.len\n    \n    def __getitem__(self, idx):\n        real_idx = self.shuffledidx[idx]\n        img_path = self.img_files[real_idx]\n        img_name = self.img_files[real_idx].split(\"/\")[-1][:-4]\n        \n        img = cv2.imread(img_path)\n        img = Image.fromarray(img)\n        details = df.loc[df[\"image_name\"] == img_name]\n        label = details['target'].values\n        details = details.drop('image_name', axis=1)\n        details = details.values.reshape(-1,)\n\n        # apply transforms on image\n        if self.transforms is not None:\n            img = self.transforms(img)\n        \n        return (img, details, label)","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test the class"},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms = T.Compose(\n    [\n        T.Resize((600, 600)),\n        T.RandomHorizontalFlip(),\n        T.RandomCrop(224),\n        T.ToTensor(),\n        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ]\n)\n\nrawdataset = SIIMTrainDataset(root_to_train_img, df, transforms, True)\nidx = random.choice(list(range(len(rawdataset))))\nimg, details, label = rawdataset.__getitem__(idx)\nprint(img.shape)\nprint(details.shape)\nprint(label.shape)","execution_count":17,"outputs":[{"output_type":"stream","text":"33126\ntorch.Size([3, 224, 224])\n(20,)\n(1,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataloader = DataLoader(rawdataset, batch_size=64, shuffle=True)\nimg, details, label = next(iter(dataloader))\nprint(img.shape)\nprint(details.shape)\nprint(label.shape)","execution_count":18,"outputs":[{"output_type":"stream","text":"torch.Size([64, 3, 224, 224])\ntorch.Size([64, 20])\ntorch.Size([64, 1])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"Thought out architecture\n\nConvModel --> Feature Extractor\nFlatten the output of ConvModel\n\nDenseModel --> Feature Extractor from linear details\n\nDenseModel2 --> Concat out of Conv and DenseModel"},{"metadata":{},"cell_type":"markdown","source":"## Conv Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic Conv Model\n# class ConvModel(nn.Module):\n#     def __init__(self, input_c):\n#         super(ConvModel, self).__init__()\n        \n#         blocks = []\n#         blocks.append(nn.Conv2d(input_c, 64, (3,3)))\n#         in_c = 64\n#         for _ in range(3):\n#             blocks.append(nn.Conv2d(in_c, in_c*2, 7))\n#             blocks.append(nn.BatchNorm2d(in_c*2))\n#             blocks.append(nn.ReLU(inplace=True))\n#             blocks.append(nn.MaxPool2d(3))\n#             in_c = in_c*2\n            \n#         for _ in range(2):\n#             blocks.append(nn.Conv2d(in_c, in_c, 3, padding=0))\n#             blocks.append(nn.BatchNorm2d(in_c))\n#             blocks.append(nn.ReLU(inplace=True))\n        \n#         blocks.append(nn.Flatten())\n#         self.net = nn.Sequential(*blocks)\n        \n#     def forward(self, x):\n#         return self.net(x)\n\n    \n# Pretrained Conv Model\nfrom torchvision.models import resnet50\nconv = resnet50(pretrained=True)\nfor i in conv.parameters():\n    i.requires_grad = False\n\nblocks = list(conv.children())[:-3][:-1]\n\nclass ConvModel(nn.Module):\n    def __init__(self, blocks):\n        super(ConvModel, self).__init__()\n        \n        blocks.append(nn.Conv2d(512, 512, 7))\n        blocks.append(nn.BatchNorm2d(512))\n        blocks.append(nn.MaxPool2d(3))\n        \n        blocks.append(nn.Conv2d(512, 512, 3))\n        blocks.append(nn.BatchNorm2d(512))\n        blocks.append(nn.MaxPool2d(3))\n        \n        blocks.append(nn.Flatten())\n\n\n        self.net = nn.Sequential(*blocks)\n    \n    def forward(self, x):\n        return self.net(x)\n    \n    \n# Dense Model 1\nclass DenseModelA(nn.Module):\n    def __init__(self, input_dim):\n        super(DenseModelA, self).__init__()\n        \n        blocks = []\n        blocks.append(nn.Linear(input_dim, 64))\n        blocks.append(nn.ReLU(inplace=True))\n        in_c = 64\n        for _ in range(3):\n            blocks.append(nn.Linear(in_c, in_c*2))\n            blocks.append(nn.ReLU(inplace=True))\n            in_c = in_c*2\n        \n        self.net = nn.Sequential(*blocks)\n        \n    def forward(self, x):\n        return self.net(x)\n    \n# Dense Model 2\nclass DenseModelB(nn.Module):\n    def __init__(self, input_dim):\n        super(DenseModelB, self).__init__()\n        \n        blocks = []\n        while input_dim >4:\n            blocks.append(nn.Linear(input_dim, input_dim//4))\n            blocks.append(nn.ReLU(inplace=True))\n            input_dim = input_dim // 4\n                \n        blocks.append(nn.Linear(input_dim, 1))\n        blocks.append(nn.Softmax(dim=-1))\n        \n        self.net = nn.Sequential(*blocks)\n    \n    def forward(self, x):\n        return self.net(x)\n\n# Final Model\nclass FinalModel(nn.Module):\n    def __init__(self, conv, denseA, denseB):\n        super(FinalModel, self).__init__()\n        \n        self.conv = conv\n        self.denseA = denseA\n        self.denseB = denseB\n        \n    def forward(self, x, y):\n        x = self.conv(x)\n        y = self.denseA(y)\n        out = x + y\n        out = self.denseB(out)\n        return out","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conv = ConvModel(blocks)\ndenseA = DenseModelA(20)\ndenseB = DenseModelB(512)\n\nif torch.cuda.is_available():\n    conv = conv.cuda()\n    denseA = denseA.cuda()\n    denseB = denseB.cuda()\n\nfinal = FinalModel(conv, denseA, denseB)\n\nif torch.cuda.is_available():\n    final = final.cuda()","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in denseA.parameters():\n    print(i.size())","execution_count":21,"outputs":[{"output_type":"stream","text":"torch.Size([64, 20])\ntorch.Size([64])\ntorch.Size([128, 64])\ntorch.Size([128])\ntorch.Size([256, 128])\ntorch.Size([256])\ntorch.Size([512, 256])\ntorch.Size([512])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Training loop"},{"metadata":{},"cell_type":"markdown","source":"Try with a common optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\noptimizer = optim.Adam(final.parameters(), lr=1e-4)\ncriterion = nn.BCELoss()\ntot_time = 0\nfor _ in range(10):\n    running_loss = 0.0\n    for batch, (img, details, label) in enumerate(dataloader):\n        start = time()\n        \n        img = img.float()\n        details = details.float()\n        label = label.float()\n        \n        if torch.cuda.is_available():\n            img = img.cuda()\n            details = details.cuda()\n            label = label.cuda()\n        \n        optimizer.zero_grad()\n        \n        pred = final(img, details)\n        loss = criterion(pred, label)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        tot_time += time()-start\n        if batch % 10 == 9:\n            print(f\"Done [{batch+1}/{len(dataloader)}] batches time required = {tot_time}\")\n            tot_time = 0\n    \n    print(f\"epoch = {_} loss = {running_loss / len(rawdataset)}\")","execution_count":22,"outputs":[{"output_type":"stream","text":"Done [10/518] batches time required = 1.0577387809753418\nDone [20/518] batches time required = 1.0494811534881592\nDone [30/518] batches time required = 1.0569958686828613\nDone [40/518] batches time required = 1.0623228549957275\nDone [50/518] batches time required = 1.0524938106536865\nDone [60/518] batches time required = 1.0711050033569336\nDone [70/518] batches time required = 1.0590739250183105\nDone [80/518] batches time required = 1.0533688068389893\nDone [90/518] batches time required = 1.0721418857574463\nDone [100/518] batches time required = 1.0594682693481445\nDone [110/518] batches time required = 1.0550408363342285\nDone [120/518] batches time required = 1.0626990795135498\nDone [130/518] batches time required = 1.0548985004425049\nDone [140/518] batches time required = 1.0730366706848145\nDone [150/518] batches time required = 1.057999610900879\nDone [160/518] batches time required = 1.0556516647338867\nDone [170/518] batches time required = 1.055018663406372\nDone [180/518] batches time required = 1.069779634475708\nDone [190/518] batches time required = 1.0615262985229492\nDone [200/518] batches time required = 1.0515637397766113\nDone [210/518] batches time required = 1.0487189292907715\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-28401bd1e99b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-547b7b55f062>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreal_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}